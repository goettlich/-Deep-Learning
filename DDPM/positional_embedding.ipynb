{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embeddings\n",
    "\n",
    "This is a small visualization of time embeddings.\n",
    "\n",
    "Good ressources: Time embeddings added instead of concatenated\n",
    "* https://ai.stackexchange.com/questions/35990/why-are-embeddings-added-not-concatenated\n",
    "* https://mathoverflow.net/questions/248466/why-are-two-random-vectors-in-mathbb-rn-approximately-orthogonal-for-large\n",
    "* https://www.reddit.com/r/MachineLearning/comments/cttefo/comment/exs7d08/?context=3\n",
    "* https://medium.com/@waelrashwan/demystifying-transformer-architecture-the-magic-of-positional-encoding-5fe8154d4a64\n",
    "* Desmos: https://www.desmos.com/calculator/nvpbogxcnd?lang=de\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    \n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "temb_dim = 128\n",
    "all_time_steps_considered = torch.arange(0,500)\n",
    "t_emb = get_time_embedding(all_time_steps_considered, temb_dim)\n",
    "\n",
    "x = np.arange(t_emb.shape[1])\n",
    "cmap = 'magma'\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 10), gridspec_kw={'height_ratios': [3, 1], 'hspace': 0.3})\n",
    "\n",
    "# Plot heatmap\n",
    "cax = ax1.imshow(t_emb.cpu().numpy(), aspect='auto', cmap=cmap, extent=[x.min(), x.max(), 0, t_emb.shape[0]])\n",
    "ax1.set_xlabel('x-axis')\n",
    "ax1.set_ylabel('Embeddings')\n",
    "ax1.set_title('Embeddings Visualization')\n",
    "\n",
    "# Plot each line with a different color from the colormap\n",
    "cmap = cm.get_cmap(cmap, t_emb.shape[0])\n",
    "plot_every = 50\n",
    "for i in range(0, t_emb.shape[0], plot_every):\n",
    "    ax2.plot(x, t_emb[i].cpu().numpy(), color=cmap(i))\n",
    "ax2.set_xlabel('x-axis')\n",
    "ax2.set_ylabel(f'Embedding values of every {plot_every}th value')\n",
    "ax2.set_title('Embeddings Visualization')\n",
    "\n",
    "# Adjust layout to make room for colorbar\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(cax, cax=cbar_ax, label='Embedding values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Enable inline plotting in Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    \n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb\n",
    "\n",
    "# Parameters\n",
    "temb_dim = 64\n",
    "time_steps = torch.arange(0, 100)\n",
    "t_emb = get_time_embedding(time_steps, temb_dim)\n",
    "\n",
    "x = np.arange(t_emb.shape[1])\n",
    "cmap = 'magma'\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 10), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "# heatmap of embedding\n",
    "cax = ax1.imshow(t_emb.cpu().numpy(), aspect='auto', cmap=cmap, extent=[x.min(), x.max(), 0, t_emb.shape[0]])\n",
    "cbar = fig.colorbar(cax, ax=[ax1, ax2], location='right', pad=0.05)\n",
    "cbar.set_label('Embedding values')\n",
    "ax1.set_xlabel('Embedding dimensions')\n",
    "ax1.set_ylabel('Timesteps')\n",
    "ax1.set_title('Embeddings Visualization')\n",
    "\n",
    "line = ax1.axhline(0, color='white', linewidth=2)\n",
    "plot_line, = ax2.plot([], [], lw=2)\n",
    "ax2.set_xlim(0, temb_dim)\n",
    "ax2.set_ylim(torch.min(t_emb).item(), torch.max(t_emb).item())\n",
    "ax2.set_xlabel('Embedding dimensions')\n",
    "ax2.set_ylabel('Embedding value')\n",
    "ax2.set_title('Embedding Vector for Current Timestep')\n",
    "\n",
    "progress_bar = tqdm(total=len(time_steps), desc=\"Generating Frame for Timestep\")\n",
    "\n",
    "def update(frame):\n",
    "    # Update horizontal line position on heatmap\n",
    "    line.set_ydata([frame, frame])\n",
    "    plot_line.set_data(x, t_emb[frame].cpu().numpy())\n",
    "    progress_bar.update(1)\n",
    "    progress_bar.set_description(f\"Generating Frame for Timestep [{frame} / {len(time_steps)}\")\n",
    "    \n",
    "    return line, plot_line\n",
    "\n",
    "# Animation (takes a while)\n",
    "ani = FuncAnimation(fig, update, frames=np.arange(0, len(time_steps)), blit=True, interval=100, repeat=False)\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
